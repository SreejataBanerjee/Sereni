import torch
from transformers import AutoModelForSeq2SeqLM, AutoTokenizer
from datasets import load_dataset
from torch.utils.data import Dataset
from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments

# Load model and tokenizer globally to avoid reloading on each request
model_name = "microsoft/GODEL-v1_1-base-seq2seq"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Fine-tune the model (optional, only for training)
def fine_tune_model(model, tokenizer):
    """
    Fine-tunes the chatbot model using the Empathetic Dialogues dataset.
    """
    dataset = load_dataset("bdotloh/empathetic-dialogues-contexts", trust_remote_code=True)

    # Preprocessing function
    def preprocess_data(example):
        user_input = example.get("context", "")
        bot_response = example.get("utterance", "")
        input_text = f"User: {user_input} Bot:"
        target_text = bot_response
        return {
            "input_ids": tokenizer.encode(input_text, truncation=True, max_length=512),
            "labels": tokenizer.encode(target_text, truncation=True, max_length=512)
        }

    # Apply preprocessing
    train_dataset = dataset["train"].map(preprocess_data, remove_columns=dataset["train"].column_names)
    val_dataset = dataset["validation"].map(preprocess_data, remove_columns=dataset["validation"].column_names)

    # Training arguments
    training_args = Seq2SeqTrainingArguments(
        output_dir="./results",
        evaluation_strategy="epoch",
        learning_rate=2e-5,
        per_device_train_batch_size=4,
        num_train_epochs=1,
        weight_decay=0.01,
        save_total_limit=2,
        logging_dir="./logs"
    )

    # Trainer
    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
        tokenizer=tokenizer
    )

    # Train
    trainer.train()
    return model

# Generate chatbot response
def generate_response(user_input, chat_history=""):
    """
    Generates an empathetic response from the chatbot.
    
    Parameters:
        user_input (str): The user's message.
        chat_history (str): Previous conversation context.
    
    Returns:
        str: Chatbot's response.
    """
    empathetic_prompt = f"You are an empathetic mental health assistant. Be kind and supportive. User says: '{user_input}'"
    context = f"{chat_history} User: {user_input}"
    
    inputs = tokenizer.encode(empathetic_prompt + " " + context, return_tensors="pt", truncation=True)
    outputs = model.generate(inputs, max_length=1000, top_k=50, top_p=0.95, temperature=0.7)
    
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

# Apply tailored responses for emotional support
def tailored_responses(user_input, model_response):
    """
    Customizes responses based on user's emotional state.
    
    Parameters:
        user_input (str): The user's input.
        model_response (str): Response generated by the model.
    
    Returns:
        str: Final chatbot response.
    """
    user_input_lower = user_input.lower()
    if "sad" in user_input_lower:
        return "I'm sorry you're feeling this way. It's okay to feel sad sometimes. Do you want to talk more about what's troubling you?"
    elif "anxious" in user_input_lower:
        return "Anxiety can be overwhelming. Try taking a few deep breaths with me. I'm here to help."
    elif "happy" in user_input_lower:
        return "That's wonderful to hear! What made you happy today?"
    else:
        return model_response

# Flask API call function
def chat(user_input, chat_history=""):
    """
    Handles chat requests from the Flask server.
    
    Parameters:
        user_input (str): The user's message.
        chat_history (str): The previous chat history.
    
    Returns:
        str: The chatbot's response.
    """
    model_response = generate_response(user_input, chat_history)
    final_response = tailored_responses(user_input, model_response)
    
    return final_response
